%\documentclass[a4paper,12pt]{article}
\documentclass[12pt]{article}
%\usepackage{times}
\usepackage{mathptmx}
\renewcommand{\ttdefault}{cmtt}
\usepackage{graphicx}

\usepackage[pdftex,
bookmarks,
bookmarksopen,
pdfauthor={David Clayton},
pdftitle={Imputed SNP analyses with snpMatrix}]
{hyperref}

\setlength{\topmargin}{-20mm}
\setlength{\headheight}{5mm}
\setlength{\headsep}{15mm}
\setlength{\textheight}{245mm}
\setlength{\oddsidemargin}{10mm}
\setlength{\evensidemargin}{0mm}
\setlength{\textwidth}{150mm}
\title{Imputed SNP analyses with snpMatrix}
\author{David Clayton}
\date{\today}

\usepackage{Sweave}
\SweaveOpts{echo=TRUE, pdf=TRUE, eps=FALSE}

\begin{document}
\setkeys{Gin}{width=1.0\textwidth}
%\VignetteIndexEntry{imputation}
%\VignettePackage{snpMatrix}

\maketitle

% R code as
%<<label[,fig=TRUE]>>=
%
%@ 

\section*{Getting started}
The need for imputation in SNP analysis studies occurs when we have a
smaller set of samples in which a large number of SNPs have been
typed, and a larger set of samples typed in only a subset of the
SNPs. We use the smaller, complete dataset (which will be termed the
{\em training dataset}) to impute the missing SNPs in the larger,
incomplete dataset (the {\em target dataset}). Examples of such applications 
include:
\begin{itemize}
\item use of HapMap data to impute association tests for a large
  number of SNPs, given data from genome-wide studies using, for
  example, a 500K SNP array, and
\item meta-analyses which seek to combine results from two platforms
  such as the Affymetrix 500K and Illumina 550K platforms.
\end{itemize}
Here we will not use a real example such as the above to explore the
use of {\tt snpMatrix} for imputation, 
but generate a fictitious example using the
data analysed in earlier exercises. This is particularly artificial in
that we have seen that these data suffer from extreme heterogeneity of 
population structure. 

We start by attaching the required libraries and accessing the data
used in the exercises:

<<init>>=
library(snpMatrix)
library(hexbin)
data(for.exercise)
@ 

Next we select alternate SNPs to be potentially missing or present 
in the target dataset:

<<select>>= 
sel <- seq(1, ncol(snps.10),2) 
missing <- snps.10[,sel]
present <- snps.10[,-sel] 
missing 
present 
@

We also need to know where the SNPs are on the chromosome in order to
avoid having to search the entire chromosome for suitable predictors
of a missing SNP:

<<positions>>=
pos.miss <- snp.support$position[sel]
pos.pres <- snp.support$position[-sel]
@ 

\section*{Calculating the imputation rules}

The next step is to calculate a set of regression equations which
provide rules for imputing the {\tt missing} SNPs from the {\tt
  present} SNPs. This is carried out by the function {\tt snp.imputation}:

<<rules>>=
rules <- snp.imputation(present, missing, pos.pres, pos.miss)
@ 

This took a short while. But the wait was really quite short when we
consider what the function has done. For each of the 14,251 SNPs in
the ``missing'' set, the function has performed a forward step-wise
regression on the 50 nearest SNPs in the ``present'' set, stopping each
search either when the $R^2$ for prediction exceeds 0.9 or after
including 4 SNPs in the regression. The figures 50, 0.9 and 4 in the
previous sentence are the default values of the function arguments {\tt try},
{\tt r2.stop}, and {\tt max.X}. Each element of {\tt rules} contains a
regression equation  together with the $R^2$ achieved:

<<rule1>>=
rules[1]
@ 

A summary table of all the 14,251 regression equations is generated by 

<<summary>>=
summary(rules)
@ 

Columns represent the number of SNPs in the regression and rows
represent grouping on $R^2$. The first column (headed {\tt 0})
represents SNPs which were monomorphic in the sample. The same
information may be displayed graphically by

<<ruleplot,fig=TRUE>>=
plot(rules)
@ 

\section*{Carrying out the association tests}

The association tests for imputed SNPs can be carried out using the
function {\tt single.snp.tests}. 

<<imptest>>=
imp <- single.snp.tests(cc, stratum, data=subject.support,
                        snp.data=present, rules=rules)
@ 

Using the observed data in the matrix {\tt present} and the set of
imputation rules stored in {\tt rules}, the above
command  imputes each of the imputed SNPs, carries out 1- and 2-df
single tests for association,  returns the results in the object {\tt
  imp}. To see how successful imputation has been, we can carry out
the same tests using the {\em true} data in {\tt missing}:

<<realtest>>= 
obs <- single.snp.tests(cc, stratum, data=subject.support, snp.data=missing)
@ 

The next commands extract the $p$-values for the 1-df tests, using both
the  imputed and the true ``missing'' data, and plot one against the
other (using the {\tt hexbin} plotting package for clarity):

<<compare,fig=TRUE>>=
logP.imp <- -log10(p.value(imp, df=1))
logP.obs <- -log10(p.value(obs, df=1))
hb <- hexbin(logP.obs, logP.imp, xbin=50)
sp <- plot(hb)
hexVP.abline(sp$plot.vp, 0, 1, col="black")
@ 

As might be expected, the agreement is rather better if we only
compare the results for SNPs that can be computed with high $R^2$.The
$R^2$ value could be extracted from the {\tt rules} object, but below
it is obtained by dividing the ``effective'' sample size ({\tt N.r2})
by the true sample size ({\tt N}) in the test results for imputed SNPs.

<<best,fig=TRUE>>=
r2 <- imp@N.r2/imp@N
hb <- hexbin(logP.obs[r2>0.9], logP.imp[r2>0.9], xbin=50)
sp <- plot(hb)
hexVP.abline(sp$plot.vp, 0, 1, col="black")
@ 

\end{document}
